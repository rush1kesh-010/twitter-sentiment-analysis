{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "# 1. Train Logistic Regression.\n",
    "# 2. Train BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "Accuracy: 0.779303125\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77    160000\n",
      "           1       0.77      0.80      0.78    160000\n",
      "\n",
      "    accuracy                           0.78    320000\n",
      "   macro avg       0.78      0.78      0.78    320000\n",
      "weighted avg       0.78      0.78      0.78    320000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[121188  38812]\n",
      " [ 31811 128189]]\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression\n",
    "\n",
    "# Load processed training and test data\n",
    "X_train_vect = load_npz('../data/processed/X_train_vect.npz')\n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "X_test_vect = load_npz('../data/processed/X_test_vect.npz')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "log_reg = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "log_reg.fit(X_train_vect, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_log_reg = log_reg.predict(X_test_vect)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log_reg)}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1280000 training samples and 320000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('../data/processed/X_train.npy', allow_pickle=True)  \n",
    "X_test = np.load('../data/processed/X_test.npy', allow_pickle=True)    \n",
    "y_train = np.load('../data/processed/y_train.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')  \n",
    "\n",
    "# Check data loading\n",
    "print(f\"Loaded {len(X_train)} training samples and {len(X_test)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1280000/1280000 [24:54<00:00, 856.44 examples/s] \n",
      "Map: 100%|██████████| 320000/320000 [08:35<00:00, 620.20 examples/s]\n",
      "Saving the dataset (3/3 shards): 100%|██████████| 1280000/1280000 [00:15<00:00, 84847.91 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 320000/320000 [00:04<00:00, 71524.30 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization function for preparing data for BERT model\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Convert data to Hugging Face's Dataset format\n",
    "train_data = Dataset.from_dict({'text': X_train, 'label': y_train})\n",
    "test_data = Dataset.from_dict({'text': X_test, 'label': y_test})\n",
    "\n",
    "# Tokenize the data (this will be done on the 'text' column of the dataset)\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Save the tokenized data\n",
    "train_data.save_to_disk('../data/processed/train_data_bert')\n",
    "test_data.save_to_disk('../data/processed/test_data_bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'))\n",
    "train_data = Dataset.load_from_disk('../data/processed/train_data_bert')\n",
    "test_data = Dataset.load_from_disk('../data/processed/test_data_bert')\n",
    "\n",
    "# Load BERT\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=8,  \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_data,           \n",
    "    eval_dataset=test_data,             \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.predictions.argmax(axis=-1), p.label_ids)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_data)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)  # Convert logits to predicted class labels\n",
    "y_true = predictions.label_ids  \n",
    "\n",
    "accuracy = (y_pred == y_true).mean()\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"BERT Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
